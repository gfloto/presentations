# Presentations

This repo contains presentations I've given on machine learning topics in the last 2 years.

# D3M Lab
The majority of my presentations were a part of my computer science MSc in the [Data-Drive Decision Making Lab (D3M)](https://d3m.mie.utoronto.ca/).

### Diffusion 1
Presentation [here](https://docs.google.com/presentation/d/1aOW4_gISJFMDhKTDm9htoTLDSEno8VlIyWrRdwA3WcA/edit?usp=sharing). In-depth presentation of diffusion models, primarily focusing on the [DDPM](https://arxiv.org/abs/2006.11239) paper. Some connections are made to the [continuous SDE intepretation](https://arxiv.org/abs/2011.13456),.

### Diffusion 2
Presentation [here](https://docs.google.com/presentation/d/1j8gcOAHXmCAiCSsXTxrv_gCMJOmT7wd3M99Y5gBdmWs/edit?usp=sharing). A secondary presentation more focused on the continuous SDE interpretation of diffusion models in the first part of the presentation. The second part deals with practical applications and generic diffusion tasks such as denoising, inpainting, and super-resolution.

### Geometry and Topology in Deep Learning
Presentation [here](https://docs.google.com/presentation/d/13xJDpNfKIbqFELHLyZ-qTdKaf1hxSmfqZhTmJ5QjQkU/edit?usp=sharing). This presentation is a more general splash of topics. First, some basic concepts related to manifolds and visualization warm-ups. Intuition behind dimension estimation methods is introduced. Familiarity with the significance of the manifolds hypothesis is assumed here. Part 2 deals with constrains with allow us to determine an embedded sub-manifold that we can be certain our data manifold is contrained to. Part 3 deals with symmetries with further describe and constrain an embedded sub-manifold which our dataset must live on. Notions of invariance and equivariance are briefly discussed. Finally, the presentation concludes with some personal thoughts on the relationship between the MLE persepective behind modern generative modelling and it's relationship to the manifold hypothesis.

### Differential Equations and Machine Learning
Presentation [here](https://docs.google.com/presentation/d/1-wL6kMeOhaRJtBSwSPcL426E01_nI-9eUIu4th125Rg/edit?usp=sharing). The presentation begins with an introduction to solving ODEs numerically, along with some different types of ODE solvers. Neural ODEs are discussed as a continuous limit of residual networks. The adjoint method and details are avoided. Flow matching and continuous normalizing flows are discussed in relation to diffusion models. The presentation concludes with a brief discussion on graph neural networks and continuous generalizations.

### Tilted Prior
Presentation [here](https://docs.google.com/presentation/d/1Y7fUXR2YVHlOI86Qr7pM7EqDtuFtmf2EQQJIdRDXxtI/edit?usp=sharing). A rough overview of a [paper](https://openreview.net/forum?id=YlGsTZODyjz) I worked on. Problems related to the calibration of likelihood-based generative models are covered. A hypothesis related to the behaviour of the Gaussian distribution is high dimensions is introduced, and the tilted prior is proposed as a remedy. Finally, the "will-it-move" test is discussed as an additional method to help with out-of-distribution detection.

### State-Space Models
Presentation [here](https://docs.google.com/presentation/d/1RQ5JUEEfUkrzRH2BPzlfzH_93pENRdGkv4Z_bOQRe9g/edit?usp=sharing). A review of the [HiPPO](https://arxiv.org/abs/2008.07669), [S4](https://arxiv.org/abs/2111.00396) and [Mamba](https://arxiv.org/abs/2312.00752) papers, currently a hot topic in recurrent neural networks.


# EleutherAI Diffusion Reading Group

I have also been active in the [EleutherAI Diffusion Reading Group](https://github.com/tmabraham/diffusion_reading_group). I gave a presentation of [flow matching](https://docs.google.com/presentation/d/1LW7nF29dDXXpoz1kVl5bbqQl2k8kU1U2Z6SD53Uc38c/edit?usp=sharing) and [diffusion distillation](https://docs.google.com/presentation/d/1vrl7jGFabNqLQaU1DYCae7hhwY_pl4u_094LfHG7A_4/edit?usp=sharing)
